{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **By Timofei Polivanov, Sami Rahali**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "```\n",
    "\n",
    "This is a correct result (positive). \"love\" is a strongly positive word, and this is a simple and positive sentence.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "```\n",
    "\n",
    "Correct result (negative). \"don't\" is a negation that flips the positive sentiment of \"love\" into negative.\n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "```\n",
    "\n",
    "VADER recognizes \":-)\" as a positive emoticon, so it intensifies the positiveness. Correct result (positive).\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "```\n",
    "\n",
    "Correct result (negative). The word \"ruins\" has a negative score in the lexicon, as it associates with destruction. So the sentence is flagged as negative.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "```\n",
    "\n",
    "This result is slighlty incorrect (positive). \"not\" negates “ruins”, which is negative, and can become either positive or neutral. In this case it should be neutral,\n",
    "but due to ambiguity VADER classifies as positive, potentially because of the booster word “certainly”.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "```\n",
    "\n",
    "Incorrect result (negative). VADER likely assigns it negative sentiment because word 'lies' can either mean deception or reclining depending on context. So in this case it's misclassified due to lexical ambiguity.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n",
    "\n",
    "Slightly incorrect result (positive). In this case the sentence expresses that the houses aren't anything special, so it should be neutral. However, VADER classifies as positive, probably misinterpreting the meaning of the word 'like' in this context, as it can mean either 'to enjoy' or 'to resemble'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'negative', 'text_of_tweet': \"@DisavowTrump20 @RepJasmine She's a narcissist and a foul mouth idiot!\", 'tweet_url': 'https://twitter.com/i/web/status/1919382851948068930'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < -0.05:\n",
    "        return 'negative'\n",
    "        \n",
    "    if compound > 0.05:\n",
    "        return 'positive'\n",
    "    \n",
    "    return 'neutral'\n",
    "    \n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "# assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.06}) == 'positive'\n",
    "# assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.06}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE She's a narcissist and a foul mouth idiot!\n",
      "INPUT TO VADER ['@DisavowTrump20', '@RepJasmine', 'she', 'be', 'a', 'narcissist', 'and', 'a', 'foul', 'mouth', 'idiot', '!']\n",
      "VADER OUTPUT {'neg': 0.31, 'neu': 0.69, 'pos': 0.0, 'compound': -0.5562}\n",
      "#1\n",
      "\n",
      "INPUT SENTENCE I'm sorry if I disturbed you.\n",
      "INPUT TO VADER ['@drp825', '_', 'I', 'meet', 'a', 'great', 'blogger', '.', '\\n', 'he', 'mainly', 'trade', 'Tesla', ',', 'Nvidia', ',', 'Apple', ',', 'Google', ',', 'and', 'have', 'a', 'very', 'good', 'grasp', 'of', 'the', 'buying', 'and', 'sell', 'price', '.', 'if', 'it', 'help', 'you', ',', 'you', 'can', 'contact', '\\n', 'https://t.co/p8L9pX0KDa', '\\n', 'I', 'be', 'sorry', 'if', 'I', 'disturb', 'you', '.']\n",
      "VADER OUTPUT {'neg': 0.091, 'neu': 0.682, 'pos': 0.227, 'compound': 0.7902}\n",
      "#2\n",
      "\n",
      "INPUT SENTENCE @haleyrumbles Man 2025 is a wack timeline\n",
      "INPUT TO VADER ['@haleyrumble', 'Man', '2025', 'be', 'a', 'wack', 'timeline']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "#3\n",
      "\n",
      "INPUT SENTENCE 🦵🥱♿\n",
      "INPUT TO VADER ['@yobinoko1981', 'maybe', 'God', 'want', 'we', 'to', 'meet', 'a', 'few', 'wrong', 'people', 'before', 'meet', 'the', 'right', 'one', ',', 'so', 'that', 'when', 'we', 'finally', 'meet', 'the', 'person', ',', 'we', 'will', 'know', 'how', 'to', 'be', 'grateful.', '🦵', '🥱', '♿']\n",
      "VADER OUTPUT {'neg': 0.087, 'neu': 0.732, 'pos': 0.18, 'compound': 0.3182}\n",
      "#4\n",
      "\n",
      "INPUT SENTENCE Her dog also gets a Pepsid pill 30 minutes before eating.\n",
      "INPUT TO VADER ['@milalovesjoe', '@data_republican', 'same', 'for', 'my', 'daughter', 'dog', '.', 'it', 'do', 'help', 'a', 'lot', '.', 'her', 'dog', 'also', 'get', 'a', 'Pepsid', 'pill', '30', 'minute', 'before', 'eat', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.881, 'pos': 0.119, 'compound': 0.4019}\n",
      "#5\n",
      "\n",
      "INPUT SENTENCE @FadedSolMaxi @ChimpersNFT I've got a thread dropping tomorrow giving the chimpdown on everything\n",
      "INPUT TO VADER ['@fadedsolmaxi', '@ChimpersNFT', 'I', \"'ve\", 'get', 'a', 'thread', 'drop', 'tomorrow', 'give', 'the', 'chimpdown', 'on', 'everything']\n",
      "VADER OUTPUT {'neg': 0.16, 'neu': 0.84, 'pos': 0.0, 'compound': -0.2732}\n",
      "#6\n",
      "\n",
      "INPUT SENTENCE yeah it gets annoying after beyond 2 days.\n",
      "INPUT TO VADER ['@thewaythetruthx', '@JudyM32174', 'yeah', 'kind', 'of', '!', 'I', 'do', \"n't\", 'mind', 'a', 'little', 'stubble', 'but', 'yeah', 'it', 'get', 'annoying', 'after', 'beyond', '2', 'day', '.']\n",
      "VADER OUTPUT {'neg': 0.153, 'neu': 0.645, 'pos': 0.202, 'compound': -0.1134}\n",
      "#7\n",
      "\n",
      "INPUT SENTENCE Idolatry at its finest!\n",
      "INPUT TO VADER ['@0ncebit', '@the_other_98', 'die', 'for', 'Jesus', 'not', 'a', 'man', '!', 'idolatry', 'at', 'its', 'fine', '!']\n",
      "VADER OUTPUT {'neg': 0.293, 'neu': 0.589, 'pos': 0.118, 'compound': -0.5696}\n",
      "#8\n",
      "\n",
      "INPUT SENTENCE @yobinoko1981 A handful of common sense is worth a bushel of learning.🚑🚮♣🟢🔊\n",
      "INPUT TO VADER ['@yobinoko1981', 'a', 'handful', 'of', 'common', 'sense', 'be', 'worth', 'a', 'bushel', 'of', 'learning.', '🚑', '🚮', '♣', '🟢', '🔊']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.2263}\n",
      "#9\n",
      "\n",
      "INPUT SENTENCE with added camouflage\n",
      "INPUT TO VADER ['@robwalle', 'the', 'problem', 'be', 'its', 'become', 'a', 'mosh', 'pit', 'of', 'one', 'half', 'of', 'user', 'who', 'want', 'it', 'to', 'be', 'another', 'LinkedIn', ',', 'and', 'the', 'other', 'half', 'who', 'be', 'here', 'to', 'throw', 'cake', 'in', 'people', 'face', '\\n\\n', 'this', 'combine', 'to', 'ragebait', 'post', 'where', 'its', 'basically', 'just', 'LinkedIn', 'drivel', ',', 'but', 'with', 'add', 'camouflage']\n",
      "VADER OUTPUT {'neg': 0.038, 'neu': 0.939, 'pos': 0.023, 'compound': -0.1779}\n",
      "#10\n",
      "\n",
      "INPUT SENTENCE Hide.\n",
      "INPUT TO VADER ['@krassenstein', 'I', 'know', 'lib', 'do', \"n't\", 'deserve', 'due', 'process', 'after', 'all', 'the', 'illegal', 'they', 'have', 'let', 'that', 'rape', 'and', 'murder', 'our', 'people', '.', 'like', 'in', 'the', 'old', 'west', 'due', 'process', 'be', 'a', 'Colt', '.45', 'or', 'a', 'rope', '.', 'you', 'be', 'the', 'enemy', '.', 'hide', '.']\n",
      "VADER OUTPUT {'neg': 0.345, 'neu': 0.607, 'pos': 0.047, 'compound': -0.9493}\n",
      "#11\n",
      "\n",
      "INPUT SENTENCE Are you going to believe a disbeliever?\n",
      "INPUT TO VADER ['@starrsisterr', '@jasonllevin', 'while', 'the', 'catholic', 'church', 'believe', 'most', 'closely', 'what', 'the', 'early', 'Christians', 'believe', '.', 'we', 'have', 'representative', 'for', 'everything', 'else', 'in', 'this', 'world', 'so', 'why', 'not', 'a', 'representative', 'for', 'Christ', 'till', 'he', 'return', 'the', 'second', 'time', '?', 'be', 'you', 'go', 'to', 'believe', 'a', 'disbeliever', '?']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "#12\n",
      "\n",
      "INPUT SENTENCE If RFK Jr can ban pharma ads on tv then we can see a faster collapse of the American propaganda machine.\n",
      "INPUT TO VADER ['@BasedMikeLee', 'then', 'the', 'market', 'of', 'political', 'theater', 'stop', '.', 'Congress', 'be', 'in', 'on', 'the', 'political', 'theater', 'big', 'steal', '.', 'if', 'RFK', 'Jr', 'can', 'ban', 'pharma', 'ad', 'on', 'tv', 'then', 'we', 'can', 'see', 'a', 'fast', 'collapse', 'of', 'the', 'american', 'propaganda', 'machine', '.']\n",
      "VADER OUTPUT {'neg': 0.307, 'neu': 0.693, 'pos': 0.0, 'compound': -0.9217}\n",
      "#13\n",
      "\n",
      "INPUT SENTENCE Can’t believe I fell for this\n",
      "\n",
      "I was scared for  a minute\n",
      "INPUT TO VADER ['@SuperteamEarn', 'Yoh', '!', 'ca', 'n’t', 'believe', 'I', 'fall', 'for', 'this', '\\n\\n', 'I', 'be', 'scared', 'for', ' ', 'a', 'minute']\n",
      "VADER OUTPUT {'neg': 0.225, 'neu': 0.775, 'pos': 0.0, 'compound': -0.4926}\n",
      "#14\n",
      "\n",
      "INPUT SENTENCE But yes I can't watch them together 😖\n",
      "INPUT TO VADER ['@Satabdimo', '@anumnasreemkhan', '@raomeenakshi7', 'he', 'be', 'thinking', 'of', 'his', 'career', 'and', 'want', 'to', 'work', 'with', 'well', 'know', 'fl', '.', 'also', 'think', 'he', 'sign', 'this', 'before', 'iqtadar', 'success', '.', 'doubt', 'he', \"'d\", 'sign', 'a', 'project', 'where', 'he', 'be', 'more', 'supporting', 'role', 'again', '.', 'but', 'yes', 'I', 'ca', \"n't\", 'watch', 'they', 'together', '😖']\n",
      "VADER OUTPUT {'neg': 0.035, 'neu': 0.753, 'pos': 0.212, 'compound': 0.7874}\n",
      "#15\n",
      "\n",
      "INPUT SENTENCE xx\n",
      "INPUT TO VADER ['@sheilaMol1', 'it', 'be', '&', 'amp', ';', 'it', '’s', 'be', '2', 'week', 'since', 'she', 'be', 'tell', 'the', 'mammogram', 'be', 'abnormal', ',', 'so', 'a', 'very', 'stressful', 'time', '.', 'xx']\n",
      "VADER OUTPUT {'neg': 0.162, 'neu': 0.838, 'pos': 0.0, 'compound': -0.5956}\n",
      "#16\n",
      "\n",
      "INPUT SENTENCE PLEASE REST A LOT AND RECOVER WELL!!!\n",
      "INPUT TO VADER ['@orenjimaru', '_', 'OH', 'MY', 'GOD', 'CONGRATULATIONSSSS', '!', '!', '!', '!', '!', '!', '!', 'IM', 'SO', 'HAPPY', 'for', 'you', '!', '!', '!', '!', 'please', 'rest', 'a', 'LOT', 'and', 'RECOVER', 'well', '!', '!', '!']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.464, 'pos': 0.536, 'compound': 0.9308}\n",
      "#17\n",
      "\n",
      "INPUT SENTENCE would\n",
      "INPUT TO VADER ['@_zaibii', 'Chad', 'Gable', 'be', 'a', 'great', 'in', 'ring', 'work', ',', 'not', 'Angle', 'but', 'he', '’', 'a', 'great', 'talent', '\\n\\n', 'Priest', 'have', 'come', 'up', 'through', 'the', 'game', 'so', 'be', 'what', 'it', 'be', 'he', '’', 'a', 'worker', '.', '\\n\\n', 'Fatu', 'be', 'awesome', ',', 'I', 'might', 'get', 'sick', 'of', 'he', 'but', 'for', 'now', 'amazing', '\\n\\n', 'Fuck', 'Goldberg', '-', 'Brons', 'already', 'have', 'well', 'match', 'then', 'Bill', 'ever', 'would']\n",
      "VADER OUTPUT {'neg': 0.119, 'neu': 0.554, 'pos': 0.327, 'compound': 0.9531}\n",
      "#18\n",
      "\n",
      "INPUT SENTENCE Every winger is a risk imo\n",
      "INPUT TO VADER ['@07_melle', 'no', 'idea', 'mate', 'there', 'be', 'n’t', 'a', 'clear', 'obvious', 'choice', 'for', 'I', '.', 'every', 'winger', 'be', 'a', 'risk', 'imo']\n",
      "VADER OUTPUT {'neg': 0.216, 'neu': 0.653, 'pos': 0.131, 'compound': -0.1779}\n",
      "#19\n",
      "\n",
      "INPUT SENTENCE Because it takes away your right to think and question.\n",
      "INPUT TO VADER ['@mariawirth1', 'Islam', 'be', 'a', 'hell', 'of', 'a', 'drug', '.', 'it', 'make', 'you', 'forget', 'everything', ',', 'include', 'reasoning', '.', 'because', 'it', 'take', 'away', 'your', 'right', 'to', 'think', 'and', 'question', '.']\n",
      "VADER OUTPUT {'neg': 0.236, 'neu': 0.764, 'pos': 0.0, 'compound': -0.7579}\n",
      "#20\n",
      "\n",
      "INPUT SENTENCE A true athlete plays with integrity, win or lose.🤕🌻ℹ\n",
      "INPUT TO VADER ['@AnnGutierrpear', 'a', 'true', 'athlete', 'play', 'with', 'integrity', ',', 'win', 'or', 'lose.', '🤕', '🌻', 'ℹ']\n",
      "VADER OUTPUT {'neg': 0.148, 'neu': 0.219, 'pos': 0.634, 'compound': 0.836}\n",
      "#21\n",
      "\n",
      "INPUT SENTENCE @DoceViida @elonmusk Kindly send me a follow request.🚀👋❤️\n",
      "INPUT TO VADER ['@DoceViida', '@elonmusk', 'kindly', 'send', 'I', 'a', 'follow', 'request.', '🚀', '👋', '❤', '️']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.4939}\n",
      "#22\n",
      "\n",
      "INPUT SENTENCE He still has a chance to win this but that McLaren is a rocket.\n",
      "INPUT TO VADER ['@verstappenews', 'champion', 'mentality', 'right', 'there', '.', 'he', 'still', 'have', 'a', 'chance', 'to', 'win', 'this', 'but', 'that', 'McLaren', 'be', 'a', 'rocket', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.688, 'pos': 0.312, 'compound': 0.6542}\n",
      "#23\n",
      "\n",
      "INPUT SENTENCE @CRahme79 Have a great Monday Charbel 🌷🍃☕️🍰🙋‍♀️\n",
      "INPUT TO VADER ['@crahme79', 'have', 'a', 'great', 'Monday', 'Charbel', '🌷', '🍃', '☕', '️', '🍰', '🙋', '\\u200d', '♀', '️']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6249}\n",
      "#24\n",
      "\n",
      "INPUT SENTENCE @No_liquid0 Give me a 10x\n",
      "INPUT TO VADER ['@No_liquid0', 'give', 'I', 'a', '10x']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "#25\n",
      "\n",
      "INPUT SENTENCE Parker Twiford\n",
      "INPUT TO VADER ['@VanoortE33383', 'thank', 'for', 'be', 'a', 'mentor', 'and', 'a', 'friend', ',', 'bro', '.', 'thank', 'for', 'the', 'thought', '-', 'provoke', 'content', '.', 'it', 'be', 'appreciate', '!', 'Parker', 'Twiford']\n",
      "VADER OUTPUT {'neg': 0.097, 'neu': 0.502, 'pos': 0.401, 'compound': 0.8172}\n",
      "#26\n",
      "\n",
      "INPUT SENTENCE Hes leaving on a free mate.\n",
      "INPUT TO VADER ['@konatefc', 'he', 's', 'leave', 'on', 'a', 'free', 'mate', '.']\n",
      "VADER OUTPUT {'neg': 0.141, 'neu': 0.471, 'pos': 0.388, 'compound': 0.4767}\n",
      "#27\n",
      "\n",
      "INPUT SENTENCE @MarioNawfal Nothing matters until you build a couple hundred…\n",
      "INPUT TO VADER ['@MarioNawfal', 'nothing', 'matter', 'until', 'you', 'build', 'a', 'couple', 'hundred', '…']\n",
      "VADER OUTPUT {'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'compound': -0.0191}\n",
      "#28\n",
      "\n",
      "INPUT SENTENCE It’s a privilege to vote but it has degraded to a fraudulent scam.\n",
      "INPUT TO VADER ['@WHLeavitt', '@LeaderJohnThune', 'heck', 'yeah', '!', 'it', '’', 'a', 'privilege', 'to', 'vote', 'but', 'it', 'have', 'degrade', 'to', 'a', 'fraudulent', 'scam', '.']\n",
      "VADER OUTPUT {'neg': 0.503, 'neu': 0.373, 'pos': 0.125, 'compound': -0.9208}\n",
      "#29\n",
      "\n",
      "INPUT SENTENCE 🤝\n",
      "INPUT TO VADER ['@tokenscape', 'Hello', '👋', '!', 'I', 'have', 'a', 'community', 'of', '60k', 'good', 'crypto', '$', 'investor', '🤑', 'and', '$', 'buyer', '🔥', 'on', 'my', 'X', 'and', 'Telegram', 'channel', '!', 'you', 'must', 'listen', 'to', 'my', 'plan', 'once', ';', 'DM', 'I', '!', '💌', '🤝']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.5826}\n",
      "#30\n",
      "\n",
      "INPUT SENTENCE A toddler with downs syndrome could do a better job running that nation\n",
      "INPUT TO VADER ['@BRICSinfo', 'Lmfao', '!', '🤣', '😂', 'he', 'be', 'entirely', 'turn', 'the', 'USA', 'into', 'planet', 'earth', 'punchline', '.', ' ', 'a', 'toddler', 'with', 'down', 'syndrome', 'could', 'do', 'a', 'well', 'job', 'run', 'that', 'nation']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.7088}\n",
      "#31\n",
      "\n",
      "INPUT SENTENCE Ask better.\n",
      "INPUT TO VADER ['@africanhub', '_', 'that', 'be', 'easy', '.', 'they', 'can', 'control', 'the', 'corrupt', 'bad', 'guy', 'with', 'money', '.', 'a', 'dumb', 'question', ',', 'but', 'you', 'know', 'this', '.', 'ask', 'well', '.']\n",
      "VADER OUTPUT {'neg': 0.169, 'neu': 0.654, 'pos': 0.177, 'compound': 0.0516}\n",
      "#32\n",
      "\n",
      "INPUT SENTENCE I believe in Charles, I would have given up a long long time back.\n",
      "INPUT TO VADER ['@ScuderiaFerrari', '@LewisHamilton', '@Charles_Leclerc', '@ScuderiaFerrari', 'continue', 'to', 'make', 'it', 'really', 'hard', 'to', 'support', 'they', '.', 'if', 'it', 'be', 'n’t', 'for', 'the', 'fact', 'that', 'a', ')', 'I', '’m', 'a', 'die', 'hard', 'fan', 'b', ')', 'I', 'have', 'faith', 'in', 'Fred', 'and', 'C', ')', 'I', 'believe', 'in', 'Charles', ',', 'I', 'would', 'have', 'give', 'up', 'a', 'long', 'long', 'time', 'back', '.']\n",
      "VADER OUTPUT {'neg': 0.145, 'neu': 0.695, 'pos': 0.16, 'compound': 0.1969}\n",
      "#33\n",
      "\n",
      "INPUT SENTENCE 44.908\n",
      "INPUT TO VADER ['@bycoinhunter', '@revolving_game', '@hatchcoin', '3.2', 'm', 'market', 'cap', 'for', '#', 'HATCHonBNB', 'be', 'just', 'a', 'warm', 'up', '.', 'People', 'buy', 'meme', 'on', '100', '-', '200', 'M', 'mcap', ',', 'to', 'get', '2x', 'or', 'more', '.', 'you', 'think', 'a', 'token', 'back', 'by', 'RG', 'with', 'strong', 'community', 'and', '20', 'season', 'of', 'different', 'kind', 'of', 'genre', 'gaming', 'experience', ',', 'solid', 'ip', 'ca', 'n’t', 'do', 'it', ',', 'then', 'you', 'be', 'wrong', '.', '44.908']\n",
      "VADER OUTPUT {'neg': 0.052, 'neu': 0.835, 'pos': 0.114, 'compound': 0.4019}\n",
      "#34\n",
      "\n",
      "INPUT SENTENCE 🤷\n",
      "INPUT TO VADER ['@wallstreetape', '@RepMTG', 'any', 'protest', 'should', 'be', 'make', 'in', 'the', 'vicinity', 'of', 'the', 'Town', 'Hall', ',', 'County', 'Commission', 'building', ',', 'State', 'Capitol', ',', 'US', 'Capitol', '.', 'that', 'be', 'where', 'you', 'redress', 'your', 'grievance', 'with', 'the', 'government', '.', 'not', 'in', 'the', 'middle', 'of', 'a', 'road', ',', 'in', 'front', 'of', 'several', 'business', ',', 'at', 'school', ',', 'or', 'in', 'a', 'neighborhood', '.', '🤷']\n",
      "VADER OUTPUT {'neg': 0.102, 'neu': 0.898, 'pos': 0.0, 'compound': -0.6249}\n",
      "#35\n",
      "\n",
      "INPUT SENTENCE @dxrnelljcl waiting for a runner\n",
      "INPUT TO VADER ['@dxrnelljcl', 'wait', 'for', 'a', 'runner']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "#36\n",
      "\n",
      "INPUT SENTENCE how does that work\n",
      "INPUT TO VADER ['@_JKNFT', '_', '@FrDaveNix', '@fredsimontlm', 'yes', ',', 'not', 'what', 'I', 'be', 'say', ',', 'but', 'to', 'I', 'it', 'seem', 'like', 'Fr', '.', 'Dave', 'completely', 'reject', 'Francis', 'as', 'a', 'Pope', ',', 'which', 'a', 'Catholic', 'can', 'not', 'do', 'from', 'my', 'understanding', ',', 'say', 'thing', 'about', 'Francis', 'and', 'rejectee', 'his', 'papacy', 'be', 'two', 'different', 'thing', '\\n\\n', 'so', 'how', 'do', 'that', 'work']\n",
      "VADER OUTPUT {'neg': 0.155, 'neu': 0.752, 'pos': 0.094, 'compound': -0.653}\n",
      "#37\n",
      "\n",
      "INPUT SENTENCE It doesn’t have anything to do with tariffs\n",
      "INPUT TO VADER ['@TylerThompsonAZ', '@CBSNews', 'five', 'dollar', 'for', 'a', 'bacon', 'egg', 'and', 'cheese', 'biscuit', '.', 'give', 'I', 'a', 'break', '.', 'McDonald', '’s', 'ceo', 'need', 'to', 'realize', 'that', 'the', 'problem', 'be', 'internal', '.', 'it', 'do', 'n’t', 'have', 'anything', 'to', 'do', 'with', 'tariff']\n",
      "VADER OUTPUT {'neg': 0.08, 'neu': 0.92, 'pos': 0.0, 'compound': -0.4019}\n",
      "#38\n",
      "\n",
      "INPUT SENTENCE @WHLeavitt As usual your skanky ass is Spewing BS you all are fabricating trying to cover taxpayer funds doge is diverting to line trump musk &amp; rich pockets lying your ass off leavitt figures is this the life you're going to teach your kid to be a liar\n",
      "INPUT TO VADER ['@WHLeavitt', 'as', 'usual', 'your', 'skanky', 'ass', 'be', 'Spewing', 'BS', 'you', 'all', 'be', 'fabricate', 'try', 'to', 'cover', 'taxpayer', 'fund', 'doge', 'be', 'divert', 'to', 'line', 'trump', 'musk', '&', 'amp', ';', 'rich', 'pocket', 'lie', 'your', 'ass', 'off', 'leavitt', 'figure', 'be', 'this', 'the', 'life', 'you', 'be', 'go', 'to', 'teach', 'your', 'kid', 'to', 'be', 'a', 'liar']\n",
      "VADER OUTPUT {'neg': 0.178, 'neu': 0.76, 'pos': 0.062, 'compound': -0.7717}\n",
      "#39\n",
      "\n",
      "INPUT SENTENCE 😂😂😂\n",
      "INPUT TO VADER ['@SeesinnedPro', 'Fb', 'be', 'on', 'a', 'roll', 'with', 'the', 'reel', 'I', 'be', 'excite', '😂', '😂', '😂', '😂']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'compound': 0.4767}\n",
      "#40\n",
      "\n",
      "INPUT SENTENCE A. YES\n",
      "INPUT TO VADER ['@musk_coco', 'a.', 'yes']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.4019}\n",
      "#41\n",
      "\n",
      "INPUT SENTENCE Nation First.\n",
      "INPUT TO VADER ['@PMOIndia', '\\n\\n', '@PMOIndia', 'if', 'India', 'reclaim', 'PoJK', ',', 'we', 'can', 'not', 'absorb', 'million', 'of', 'radical', '—', 'it', 'would', 'cause', 'a', 'dangerous', ',', 'permanent', 'demographic', 'shift', '.', '\\n', 'Block', 'all', 'pakistani', 'content', 'on', 'YouTube', ',', 'Instagram', '&', 'amp', ';', 'OTT', '.', '\\n', 'Ban', 'WhatsApp', 'call', '.', 'zero', 'people', '-', 'to', '-', 'people', 'contact', '—', 'permanently', '.', '\\n', 'nation', 'First', '.']\n",
      "VADER OUTPUT {'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.8625}\n",
      "#42\n",
      "\n",
      "INPUT SENTENCE jorking ur shit to keto you gotta be warped fr\n",
      "INPUT TO VADER ['@DylanB1979', '…', 'a', 'diet', '?', 'I', 'guess', 'dude', 'if', 'ur', 'like', 'a', 'freak', '.', 'jorke', 'ur', 'shit', 'to', 'keto', 'you', 'got', 'ta', 'be', 'warp', 'fr']\n",
      "VADER OUTPUT {'neg': 0.26, 'neu': 0.64, 'pos': 0.1, 'compound': -0.6124}\n",
      "#43\n",
      "\n",
      "INPUT SENTENCE @lashkid650 @iCheesurReports I often use @Dr_Psyche26736 as a backup for my Adderall refills when my regular pharmacies are out of stock due to shortages.\n",
      "INPUT TO VADER ['@lashkid650', '@icheesurreport', 'I', 'often', 'use', '@Dr_Psyche26736', 'as', 'a', 'backup', 'for', 'my', 'Adderall', 'refill', 'when', 'my', 'regular', 'pharmacy', 'be', 'out', 'of', 'stock', 'due', 'to', 'shortage', '.']\n",
      "VADER OUTPUT {'neg': 0.087, 'neu': 0.913, 'pos': 0.0, 'compound': -0.25}\n",
      "#44\n",
      "\n",
      "INPUT SENTENCE @pooL_rM311_7221 @brometheus0x yo this gesara stuff sounds like a pyramid scheme for nerds\n",
      "INPUT TO VADER ['@junia', '_', '_', '_', '@tiredofitallUSA', '@pooL_rM311_7221', '@brometheus0x', 'yo', 'this', 'gesara', 'stuff', 'sound', 'like', 'a', 'pyramid', 'scheme', 'for', 'nerd']\n",
      "VADER OUTPUT {'neg': 0.132, 'neu': 0.719, 'pos': 0.15, 'compound': 0.0772}\n",
      "#45\n",
      "\n",
      "INPUT SENTENCE @AidenHunterX Need a more permanent solution so shlomo cant go running to banks to crybully people.\n",
      "INPUT TO VADER ['@aidenhunterx', 'need', 'a', 'more', 'permanent', 'solution', 'so', 'shlomo', 'ca', 'nt', 'go', 'run', 'to', 'bank', 'to', 'crybully', 'people', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.853, 'pos': 0.147, 'compound': 0.3774}\n",
      "#46\n",
      "\n",
      "INPUT SENTENCE This is the problem with the LibTurds the care so much what the rest of the world thinks.\n",
      "INPUT TO VADER ['@langmanvince', 'who', 'give', 'a', 'fly', 'fuck', 'about', 'what', 'the', 'rest', 'of', 'the', 'world', 'think', '.', 'this', 'be', 'the', 'problem', 'with', 'the', 'LibTurds', 'the', 'care', 'so', 'much', 'what', 'the', 'rest', 'of', 'the', 'world', 'think', '.']\n",
      "VADER OUTPUT {'neg': 0.166, 'neu': 0.749, 'pos': 0.086, 'compound': -0.4588}\n",
      "#47\n",
      "\n",
      "INPUT SENTENCE fc\n",
      "INPUT TO VADER ['@topmgszn', '@FabrizioRomano', 'you', 'have', 'fixate', 'issue', '!', 'a', 'Real', 'Madrid', 'that', 'have', 'more', 'La', 'liga', ' ', 'and', 'champion', 'league', 'trophy', 'than', 'your', 'bribe', 'referee', 'Barcelona', 'fc']\n",
      "VADER OUTPUT {'neg': 0.067, 'neu': 0.778, 'pos': 0.155, 'compound': 0.5255}\n",
      "#48\n",
      "\n",
      "INPUT SENTENCE We had one come speak to us for a woman's study class\n",
      "INPUT TO VADER ['@BasedMikeLee', 'accord', 'to', 'my', 'California', 'professor', 'a', 'lesbian', '..', 'no', 'shit', 'story', '.', 'we', 'have', 'one', 'come', 'speak', 'to', 'we', 'for', 'a', 'woman', \"'s\", 'study', 'class']\n",
      "VADER OUTPUT {'neg': 0.216, 'neu': 0.784, 'pos': 0.0, 'compound': -0.7003}\n",
      "#49\n",
      "\n",
      "INPUT SENTENCE However, a CKO can drive CDO, CAIO and CIO to make systems that lead to overall wisdom in the organizations.\n",
      "INPUT TO VADER ['@mis_daily', 'a', 'CAIO', '/', 'CIO', 'or', 'a', 'CDO', 'can', 'at', 'good', 'work', 'to', 'provide', 'system', 'for', 'knowledge', 'or', 'information', '.', '\\n\\n', 'however', ',', 'a', 'cko', 'can', 'drive', 'CDO', ',', 'CAIO', 'and', 'CIO', 'to', 'make', 'system', 'that', 'lead', 'to', 'overall', 'wisdom', 'in', 'the', 'organization', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.743}\n",
      "#50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.60      0.65        25\n",
      "     neutral       0.60      0.18      0.27        17\n",
      "    positive       0.29      0.88      0.44         8\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.54      0.55      0.45        50\n",
      "weighted avg       0.61      0.50      0.49        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "count = 1\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    \n",
    "    vader_output = run_vader(the_tweet, lemmatize=to_lemmatize, verbose=1) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "\n",
    "    print(f'#{count}')\n",
    "    count += 1\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(gold, all_vader_output))\n",
    "\n",
    "\n",
    "\n",
    "# Output:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#     negative       0.71      0.60      0.65        25\n",
    "#      neutral       0.60      0.18      0.27        17\n",
    "#     positive       0.29      0.88      0.44         8\n",
    "\n",
    "#     accuracy                           0.50        50\n",
    "#    macro avg       0.54      0.55      0.45        50\n",
    "# weighted avg       0.61      0.50      0.49        50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual label, vader predicted label\n",
      "#1 negative, negative\n",
      "#2 positive, positive\n",
      "#3 negative, neutral\n",
      "#4 neutral, positive\n",
      "#5 neutral, positive\n",
      "#6 neutral, negative\n",
      "#7 negative, negative\n",
      "#8 negative, negative\n",
      "#9 neutral, positive\n",
      "#10 negative, negative\n",
      "#11 negative, negative\n",
      "#12 neutral, neutral\n",
      "#13 negative, negative\n",
      "#14 neutral, negative\n",
      "#15 neutral, positive\n",
      "#16 negative, negative\n",
      "#17 positive, positive\n",
      "#18 neutral, positive\n",
      "#19 neutral, negative\n",
      "#20 negative, negative\n",
      "#21 neutral, positive\n",
      "#22 positive, positive\n",
      "#23 positive, positive\n",
      "#24 positive, positive\n",
      "#25 neutral, neutral\n",
      "#26 positive, positive\n",
      "#27 negative, positive\n",
      "#28 negative, neutral\n",
      "#29 positive, negative\n",
      "#30 neutral, positive\n",
      "#31 negative, positive\n",
      "#32 negative, positive\n",
      "#33 negative, positive\n",
      "#34 negative, positive\n",
      "#35 negative, negative\n",
      "#36 neutral, neutral\n",
      "#37 neutral, negative\n",
      "#38 negative, negative\n",
      "#39 negative, negative\n",
      "#40 positive, positive\n",
      "#41 neutral, positive\n",
      "#42 negative, negative\n",
      "#43 negative, negative\n",
      "#44 neutral, negative\n",
      "#45 negative, positive\n",
      "#46 negative, positive\n",
      "#47 negative, negative\n",
      "#48 negative, positive\n",
      "#49 negative, negative\n",
      "#50 neutral, positive\n"
     ]
    }
   ],
   "source": [
    "# Print header\n",
    "print(\"actual label, vader predicted label\")\n",
    "\n",
    "# Print each pair of elements\n",
    "for i in range(len(gold)):\n",
    "    print(f\"#{i+1} {gold[i]}, {all_vader_output[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a:\n",
    "\n",
    "#### Scores:\n",
    "\n",
    "* Negative: Highest precision (0.71) and a high recall (0.60), meaning VADER is reliable at classifying negative tweets correctly.\n",
    "* Positive: Very high recall (0.88) but low precision (0.29), meaning VADER predicts many false positives for the positive sentiment class.\n",
    "* Neutral: Average precision (0.60) but very low recall (0.18), meaning VADER isn't very reliable at identifying neutral tweets.\n",
    "\n",
    "\n",
    "#### Most relevant scores:\n",
    "\n",
    "* The F1-score is most relevant because it takes into account both precision and recall, which is useful when the class distribution is unbalanced like in our case (only 8 positive tweets vs. 25 negative).\n",
    "* Macro average f1 (0.45) is more balanced and is more useful than accuracy (0.50), which can be skewed by class imbalance.\n",
    "* Accuracy can be misleading in this case, since the model may perform well on the majority class but badly on others.\n",
    "\n",
    "\n",
    "\n",
    "### Question 3.b:\n",
    "\n",
    "#### Sentence #3 (negative -> neutral)\n",
    "Sentence: '@haleyrumbles Man 2025 is a wack timeline'\n",
    "\n",
    "VADER's lexicon does not include the slang 'wack', so it treats that token as neutral by default. No other words carry sentiment or trigger any of the rules. As a result, the model returns a neutral score (compound 0.0) even though 'wack' signals a negative judgment in everyday usage.\n",
    "\n",
    "#### Sentence #4 (neutral -> positive)\n",
    "Sentence: '@yobinoko1981 maybe God want we to meet a few wrong people before meet the right one, so that when we finally meet the person, we will know how to be grateful. 🦵🥱♿'\n",
    "\n",
    "VADER's lexicon includes 'grateful' as a positive term and 'wrong' as negative, but the +2.0 weight on 'grateful' is higher. The emojis are also mapped to positive values. This pushes the compound to +0.3182, classifying it as positive even though the sentence is neutral.\n",
    "\n",
    "#### Sentence #5 (neutral -> positive)\n",
    "Sentence: '@MilaLovesJoe @data_republican Same for my daughters dog. It does help a lot. Her dog also gets a Pepsid pill 30 minutes before eating.'\n",
    "\n",
    "VADER's lexicon marks 'help' as +1.7 and 'a lot' functions as an intensifier, resulting in a positive compound of +0.4019. The statement is interpreted as a praise for 'help', so VADER labels it positive despite its neutral intent.\n",
    "\n",
    "#### Sentence #6 (neutral -> negative)\n",
    "Sentence: \"@FadedSolMaxi @ChimpersNFT I've got a thread dropping tomorrow giving the chimpdown on everything\"\n",
    "\n",
    "VADER's lexicon assigns a negative score to 'drop' (-1.1), so there are no positive words to balance it. In this context, 'dropping' means 'to post', however, VADER interpreted it as 'discarded' or 'let go', which has a negative connotation. So this incorrectly turns the neutral statement into a negative.\n",
    "\n",
    "#### Sentence #9 (neutral -> positive)\n",
    "Sentence: '@yobinoko1981 A handful of common sense is worth a bushel of learning.🚑🚮♣🟢🔊'\n",
    "\n",
    "VADER's lexicon treats 'learning' as positive and maps some emojis to positive weights as well. This makes VADER classify this as positive, even though it is a neutral proverb.\n",
    "\n",
    "#### Sentence #14 (neutral -> negative)\n",
    "Sentence: '@SuperteamEarn Yoh! Can't believe I fell for this\\n\\nI was scared for  a minute'\n",
    "\n",
    "VADER's lexicon flags 'scared' (-1.9) as negative and provides no positive tokens to balance. The negation 'Can't' is not realted to 'scared', so the model labels it negative, even though it is a neutral expression of surprise.\n",
    "\n",
    "#### Sentence #15 (neutral -> positive)\n",
    "Sentence: '@Satabdimo @anumnasreemkhan @raomeenakshi7 He's thinking of his career and wants to work with well know FLs. Also think he signed this before Iqtadars success. Doubt he'd sign a project where he's more supporting role again. But yes I can't watch them together 😖'\n",
    "\n",
    "VADER's emoji lexicon mistakenly treats 😖 as enthusiastic, adding positive weight. The only other word is 'yes', and the negation 'can't' carries no meaning by itself. This yields a positive label, despite the user expressing discomfort.\n",
    "\n",
    "#### Sentence #18 (neutral -> positive)\n",
    "Sentence: '@_zaibii Chad Gable is a great in ring work, not Angle but he’s a great talent\\n\\nPriest has come up through the game so is what it is he’s a worker.\\n\\nFatu is awesome, I might get sick of him but for now amazing \\n\\nFuck Goldberg - Brons already had better matches then Bill ever would'\n",
    "\n",
    "VADER's lexicon flags multiple strong positives: 'great', 'talent', 'awesome', 'amazing', and potentially applies capitalization and punctuation heuristics. These return a very high compound value. However, there is also negativity present in the sentence in terms of explicit langauge, but there are more positive words, so VADER interprets a mixed sentence as mostly positive.\n",
    "\n",
    "#### Sentence #19 (neutral -> negative)\n",
    "Sentence: '@07_melle No idea mate there isn't a clear obvious choice for me. Every winger is a risk imo'\n",
    "\n",
    "VADER's lexicon assigns 'risk' a negative score. 'imo' is not included in the lexicon and no rules apply, so the single negative results in flagging it negative although the statement is a mostly neutral opinion.\n",
    "\n",
    "#### Sentence #21 (neutral -> positive)\n",
    "Sentence: '@AnnGutierrpear A true athlete plays with integrity, win or lose.🤕🌻ℹ'\n",
    "\n",
    "VADER's lexicon tags 'win', 'true', and 'integrity' as positive, and at least one emoji is mapped positively. These signals sum to a high value, so VADER marks it positive even though the tone is neutral.\n",
    "\n",
    "#### Sentence #27 (negative -> positive)\n",
    "Sentence: '@KonateFC Hes leaving on a free mate.'\n",
    "\n",
    "VADER's lexicon treats 'free' as a positive attribute with no negative entry for the context. The single positive token results in a positive classification, misreading sarcasm as positivity.\n",
    "\n",
    "#### Sentence #28 (negative -> neutral)\n",
    "Sentence: '@MarioNawfal Nothing matters until you build a couple hundred…'\n",
    "\n",
    "VADER's lexicon scores 'nothing' as negative, but without any intensifier or contrastive conjunction rule, it results in only a weak negative compound, which is classified neutral. The strong intended negativity is lost becuse it is implicit.\n",
    "\n",
    "#### Sentence #29 (positive -> negative)\n",
    "Sentence: '@WHLeavitt @LeaderJohnThune Heck yeah! It's a privilege to vote but it has degraded to a fraudulent scam.'\n",
    "\n",
    "VADER's lexicon includes 'privilege' and 'fraudulent', 'scam'. Its contrastive-conjunction rule degrades the first clause and emphasizes what follows 'but', so the heavier negatives dominate and the compound is mostly negative. A mixed mostly positive statement becomes negative because of this.\n",
    "\n",
    "#### Sentence #30 (neutral -> positive)\n",
    "Sentence: '@TokenScape Hello 👋 ! I have a community of 60k best crypto $investors 🤑and $buyers 🔥on my X and Telegram channel! You must listen to my plan once; DM me!💌🤝'\n",
    "\n",
    "VADER's emoji lexicon directly maps the emojis to a positive. There is also a positive word 'best', and exclamation marks. Because of this, even though the message is just an ad or promotion, it is incorrectly classified as positive.\n",
    "\n",
    "#### Sentence #31 (negative -> positive)\n",
    "Sentence: '@BRICSinfo Lmfao ! 🤣😂 he's entirely turned the USA into planet earths punchline .  A toddler with downs syndrome could do a better job running that nation'\n",
    "\n",
    "The sentence includes a lot of laughing emojis, and exclamation marks. VADER's lexicon also includes 'better' and 'job' but has no entry for 'downs' or 'syndrome', so positive tokens overwhelm. So VADER misses the hateful context entirely, because it's sarcasm.\n",
    "\n",
    "#### Sentence #32 (negative -> positive)\n",
    "Sentence: '@AfricanHub_ That's easy. They can control the corrupt bad guys with money. A dumb question, but you know this. Ask better.'\n",
    "\n",
    "VADER's lexicon marks 'better' and 'easy' as positive. There are also negative words, but positive ones have a greater sum, which results in a wrong positive classification. Somehow VADER misses the explicit negativity.\n",
    "\n",
    "#### Sentence #33 (negative -> positive)\n",
    "Sentence: '@ScuderiaFerrari @LewisHamilton @Charles_Leclerc @ScuderiaFerrari continuing to make it really hard to support them. If it weren’t for the fact that A) I’m a die hard fan B) I have faith in Fred and C) I believe in Charles, I would have given up a long long time back.'\n",
    "\n",
    "VADER gets a high compound value from the tweet because of positive words: 'support', 'faith', 'fan'. However, this misses the first part of the tweet, which is negative. Thus VADER misclassifies this as positive.\n",
    "\n",
    "#### Sentence #34 (negative -> positive)\n",
    "Sentence: '@bycoinhunter @Revolving_Games @HatchCoin 3.2m market cap for #HATCHonBNB is just a warm up. People buying memes on 100-200M mcap, to get 2x or more. You think a token backed by RG with strong community and 20 season of different kinds of genres gaming experience, solid IP can’t do it, then you are wrong. 44.908'\n",
    "\n",
    "VADER analyzed the entire text: 'solid', 'strong', 'community', 'experience', and found many positive lexicon terms, resulting in a high compound. Even thouhg this tweet's purpose is to point out someone's mistake, which is negative intent, it gets classified as positive due positive words, which aren't really positive in this context.\n",
    "\n",
    "#### Sentence #45 (negative -> positive)\n",
    "Sentence: '@Junia___ @tiredofitallUSA @pooL_rM311_7221 @brometheus0x yo this gesara stuff sounds like a pyramid scheme for nerds'\n",
    "\n",
    "VADER's lexicon has a high value for 'like', which turns the sentence positive, missing the context and the insult.\n",
    "\n",
    "#### Sentence #46 (negative -> positive)\n",
    "Sentence: '@AidenHunterX Need a more permanent solution so shlomo cant go running to banks to crybully people.'\n",
    "\n",
    "VADER's lexicon tags 'solution' as positive, while 'crybully' is not included. The model flags this as positive, missing the insult and negative tone.\n",
    "\n",
    "#### Sentence #48 (negative -> positive)\n",
    "Sentence: '@TopMGSzn @FabrizioRomano You have fixated issue! A Real Madrid that have more La liga  and champions league trophies than your bribing referee Barcelona fc'\n",
    "\n",
    "VADER lexicon marks the words 'champions' and 'trophies' as positive, as well as the exclamation point. There is a balancing word 'bribing', but it doesn't outweigh the positive terms. Thus, the intended insult and negative tone are missed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import classification_report\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweets from the directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "dataset = load_files(str(airline_tweets_folder), encoding='utf-8', decode_error='replace')\n",
    "\n",
    "def call_vader(dataset, to_lemmatize=False, parts_of_speech_to_consider=None, verbose=0):\n",
    "    tweets = []\n",
    "    all_vader_output = []\n",
    "    gold = []\n",
    "\n",
    "    # Settings\n",
    "    pos = set()\n",
    "\n",
    "    label_names = dataset.target_names  # ['negative', 'neutral', 'positive']\n",
    "\n",
    "    # Process each tweet\n",
    "    for count, (tweet_text, gold_label_idx) in enumerate(zip(dataset.data, dataset.target), start=1):\n",
    "        vader_output = run_vader(tweet_text, to_lemmatize, parts_of_speech_to_consider, verbose)\n",
    "        vader_label = vader_output_to_label(vader_output)\n",
    "\n",
    "        tweets.append(tweet_text)\n",
    "        all_vader_output.append(vader_label)\n",
    "        gold.append(label_names[gold_label_idx])\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(gold, all_vader_output, labels=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.62      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.69      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default\n",
    "call_vader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.62      1750\n",
      "     neutral       0.59      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.64      0.63      0.61      4755\n",
      "weighted avg       0.65      0.62      0.61      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized\n",
    "call_vader(dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.41      0.89      0.56      1515\n",
      "    positive       0.67      0.45      0.54      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.52      0.48      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjectives\n",
    "call_vader(dataset, False, {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.21      0.34      1750\n",
      "     neutral       0.41      0.89      0.56      1515\n",
      "    positive       0.67      0.45      0.54      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.52      0.48      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized adjectives\n",
    "call_vader(dataset, True, {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.14      0.23      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.54      0.34      0.42      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.37      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nouns\n",
    "call_vader(dataset, False, {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.15      0.25      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.53      0.33      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.38      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized nouns\n",
    "call_vader(dataset, True, {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.28      0.42      1750\n",
      "     neutral       0.38      0.82      0.52      1515\n",
      "    positive       0.58      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.59      0.48      0.46      4755\n",
      "weighted avg       0.60      0.47      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbs\n",
    "call_vader(dataset, False, {'VERB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.29      0.42      1750\n",
      "     neutral       0.38      0.79      0.51      1515\n",
      "    positive       0.58      0.36      0.44      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.57      0.48      0.46      4755\n",
      "weighted avg       0.58      0.47      0.46      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized verbs\n",
    "call_vader(dataset, True, {'VERB'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we looked across all eight experiments, two things were noticeable:\n",
    "\n",
    "#### Lemmatization has virtually no impact:\n",
    "\n",
    "Full text (raw): accuracy 0.63, macro F1 0.62\n",
    "Full text (lemmatized): accuracy 0.62, macro F1 0.61\n",
    "Adjectives (raw): accuracy 0.50, macro F1 0.48\n",
    "Adjectives (lemmatized): accuracy 0.50, macro F1 0.48\n",
    "Verbs (raw): accuracy 0.47, macro F1 0.46\n",
    "Verbs (lemmatized): accuracy 0.47, macro F1 0.46\n",
    "Nouns (raw): accuracy 0.42, macro F1 0.38\n",
    "Nouns (lemmatized): accuracy 0.42, macro F1 0.38\n",
    "\n",
    "The drop from 0.63 to 0.62 accuracy (and from 0.62 to 0.61 macro F1) in the full-text run is negligible. In the parts‑of‑speech‑only runs, the raw and lemmatized versions are identical. VADER’s built‑in normalization already handles different variants of words, so extra lemmatization doesn't benefit.\n",
    "\n",
    "\n",
    "#### Parts of speech are not equally important:\n",
    "\n",
    "Adjectives only: accuracy 0.50, macro F1 0.48\n",
    "Verbs only: accuracy 0.47, macro F1 0.46\n",
    "Nouns only: accuracy 0.42, macro F1 0.38\n",
    "\n",
    "Adjectives carry the strongest sentiment. Verbs help somewhat, but many are neutral. Nouns are weakest, as most are topic markers. And also no POS subset matches the full‑text run, because VADER relies on the combination of all parts of speech plus additional rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_type='tf', min_df=2):\n",
    "    airline_vec = CountVectorizer(min_df=min_df,\n",
    "                                tokenizer=nltk.word_tokenize,\n",
    "                                stop_words=stopwords.words('english'))\n",
    "\n",
    "    airline_counts = airline_vec.fit_transform(dataset.data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "    data = airline_tfidf if data_type == 'tf' else airline_counts\n",
    "\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        data, # the selected model\n",
    "        dataset.target, # the category values for each tweet \n",
    "        test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "        ) \n",
    "\n",
    "    clf = MultinomialNB().fit(docs_train, y_train)\n",
    "    y_pred = clf.predict(docs_test)\n",
    "\n",
    "    y_test_labels = [dataset.target_names[i] for i in y_test]\n",
    "    y_pred_labels = [dataset.target_names[i] for i in y_pred]\n",
    "\n",
    "    # Output the classification report\n",
    "    print(classification_report(y_test_labels, y_pred_labels, labels=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.90      0.85       365\n",
      "     neutral       0.82      0.67      0.74       286\n",
      "    positive       0.84      0.85      0.84       300\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.81       951\n",
      "weighted avg       0.82      0.82      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf-idf, df=2\n",
    "train_model('tf', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.89      0.86       360\n",
      "     neutral       0.81      0.71      0.75       299\n",
      "    positive       0.83      0.85      0.84       292\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf-idf, df=5\n",
    "train_model('tf', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.91      0.86       353\n",
      "     neutral       0.82      0.72      0.76       309\n",
      "    positive       0.82      0.83      0.83       289\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf-idf, df=10\n",
    "train_model('tf', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87       341\n",
      "     neutral       0.87      0.73      0.79       297\n",
      "    positive       0.84      0.89      0.87       313\n",
      "\n",
      "    accuracy                           0.85       951\n",
      "   macro avg       0.85      0.84      0.84       951\n",
      "weighted avg       0.85      0.85      0.85       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bag, df=2\n",
    "train_model('bag', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87       347\n",
      "     neutral       0.82      0.73      0.77       297\n",
      "    positive       0.83      0.84      0.83       307\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bag, df=5\n",
    "train_model('bag', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.89      0.86       369\n",
      "     neutral       0.78      0.75      0.76       303\n",
      "    positive       0.84      0.79      0.81       279\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.81      0.81      0.81       951\n",
      "weighted avg       0.82      0.82      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bag, df=10\n",
    "train_model('bag', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Across all six experiments, the negative class consistently achieves the highest F1‑score (around 0.85–0.87), regardless of whether TF‑IDF or bag‑of‑words is used, and no matter which min_df value is used. Positive class come in second place (F1 around 0.83–0.87), while neutral tweets are the hardest to classify (F1 around 0.74–0.79). This pattern is true for all experiments as well. The better performance on negative and positive tweets is probably due to the fact that negative and positive language such as complaints or expressions of happines are more explicit, so the classifier can easily notice strong signal words, while neutral language is way more subtle and implicit, and doesn't use strong words.\n",
    "\n",
    "Increasing the document frequency threshold (min_df) from 2 to 5 to 10 has only a slight impact when using TF‑IDF: neutral recall increases slightly (from 0.67 to 0.72) and overall scores stay almost the same. TF‑IDF focuses less on common words and already ignores rare words, so filtering out rare words even further doesn't change much. By contrast, with raw counts when using the bag-of-words approach, raising min_df reduces performance for neutral and positive classes slightly, because Naive Bayes focuses more on rare but distinct words. So removing rare words can hurt performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)\n",
    "\n",
    "def train_model(data_type='tf', min_df=2, show_features=False, top_n=80):\n",
    "    airline_vec = CountVectorizer(min_df=min_df,\n",
    "                                  tokenizer=nltk.word_tokenize,\n",
    "                                  stop_words=stopwords.words('english'))\n",
    "\n",
    "    airline_counts = airline_vec.fit_transform(dataset.data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "    data = airline_tfidf if data_type == 'tf' else airline_counts\n",
    "\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        data,\n",
    "        dataset.target,\n",
    "        test_size=0.20\n",
    "    )\n",
    "\n",
    "    clf = MultinomialNB().fit(docs_train, y_train)\n",
    "    y_pred = clf.predict(docs_test)\n",
    "\n",
    "    y_test_labels = [dataset.target_names[i] for i in y_test]\n",
    "    y_pred_labels = [dataset.target_names[i] for i in y_pred]\n",
    "\n",
    "    print(classification_report(y_test_labels, y_pred_labels, labels=dataset.target_names))\n",
    "\n",
    "    # Show top features if requested\n",
    "    if show_features and data_type != 'tf':\n",
    "        important_features_per_class(airline_vec, clf, n=top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Volumes/Crucial/programming/vu_tm/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87       365\n",
      "     neutral       0.85      0.72      0.78       294\n",
      "    positive       0.83      0.87      0.85       292\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      "\n",
      "Important words in negative documents\n",
      "0 1484.0 @\n",
      "0 1359.0 united\n",
      "0 1214.0 .\n",
      "0 425.0 ``\n",
      "0 382.0 flight\n",
      "0 358.0 ?\n",
      "0 328.0 !\n",
      "0 324.0 #\n",
      "0 218.0 n't\n",
      "0 164.0 ''\n",
      "0 121.0 's\n",
      "0 111.0 :\n",
      "0 108.0 virginamerica\n",
      "0 108.0 service\n",
      "0 92.0 get\n",
      "0 87.0 cancelled\n",
      "0 86.0 customer\n",
      "0 85.0 plane\n",
      "0 85.0 delayed\n",
      "0 80.0 bag\n",
      "0 77.0 time\n",
      "0 77.0 -\n",
      "0 75.0 ;\n",
      "0 73.0 'm\n",
      "0 70.0 http\n",
      "0 68.0 &\n",
      "0 66.0 ...\n",
      "0 63.0 hours\n",
      "0 63.0 gate\n",
      "0 63.0 amp\n",
      "0 62.0 hour\n",
      "0 62.0 help\n",
      "0 62.0 airline\n",
      "0 59.0 ca\n",
      "0 58.0 still\n",
      "0 56.0 would\n",
      "0 56.0 late\n",
      "0 53.0 2\n",
      "0 51.0 like\n",
      "0 50.0 worst\n",
      "0 49.0 flights\n",
      "0 49.0 $\n",
      "0 46.0 delay\n",
      "0 46.0 (\n",
      "0 46.0 've\n",
      "0 44.0 really\n",
      "0 44.0 never\n",
      "0 43.0 waiting\n",
      "0 42.0 one\n",
      "0 41.0 back\n",
      "0 41.0 )\n",
      "0 40.0 us\n",
      "0 40.0 people\n",
      "0 40.0 flightled\n",
      "0 39.0 wait\n",
      "0 38.0 fly\n",
      "0 38.0 ever\n",
      "0 38.0 3\n",
      "0 37.0 thanks\n",
      "0 37.0 day\n",
      "0 36.0 lost\n",
      "0 34.0 u\n",
      "0 34.0 check\n",
      "0 33.0 due\n",
      "0 33.0 crew\n",
      "0 32.0 seat\n",
      "0 32.0 luggage\n",
      "0 32.0 got\n",
      "0 32.0 going\n",
      "0 32.0 bags\n",
      "0 31.0 last\n",
      "0 30.0 4\n",
      "0 29.0 ticket\n",
      "0 29.0 airport\n",
      "0 28.0 guys\n",
      "0 28.0 even\n",
      "0 27.0 trying\n",
      "0 27.0 today\n",
      "0 27.0 seats\n",
      "0 27.0 problems\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1415.0 @\n",
      "1 509.0 .\n",
      "1 504.0 ?\n",
      "1 314.0 jetblue\n",
      "1 290.0 :\n",
      "1 269.0 southwestair\n",
      "1 255.0 #\n",
      "1 251.0 united\n",
      "1 250.0 ``\n",
      "1 235.0 flight\n",
      "1 199.0 americanair\n",
      "1 186.0 http\n",
      "1 166.0 !\n",
      "1 163.0 usairways\n",
      "1 137.0 's\n",
      "1 79.0 get\n",
      "1 73.0 ''\n",
      "1 70.0 flights\n",
      "1 69.0 virginamerica\n",
      "1 69.0 -\n",
      "1 65.0 please\n",
      "1 61.0 )\n",
      "1 59.0 help\n",
      "1 56.0 n't\n",
      "1 56.0 (\n",
      "1 49.0 ...\n",
      "1 48.0 need\n",
      "1 46.0 dm\n",
      "1 44.0 would\n",
      "1 43.0 know\n",
      "1 42.0 us\n",
      "1 41.0 tomorrow\n",
      "1 41.0 ;\n",
      "1 40.0 fleet\n",
      "1 40.0 fleek\n",
      "1 36.0 ”\n",
      "1 35.0 “\n",
      "1 35.0 &\n",
      "1 33.0 thanks\n",
      "1 32.0 way\n",
      "1 32.0 change\n",
      "1 32.0 'm\n",
      "1 31.0 like\n",
      "1 31.0 hi\n",
      "1 31.0 could\n",
      "1 29.0 flying\n",
      "1 28.0 one\n",
      "1 28.0 fly\n",
      "1 28.0 cancelled\n",
      "1 27.0 number\n",
      "1 27.0 airport\n",
      "1 26.0 today\n",
      "1 26.0 new\n",
      "1 26.0 go\n",
      "1 25.0 check\n",
      "1 25.0 amp\n",
      "1 24.0 time\n",
      "1 23.0 travel\n",
      "1 23.0 rt\n",
      "1 23.0 destinationdragons\n",
      "1 22.0 sent\n",
      "1 22.0 make\n",
      "1 21.0 want\n",
      "1 21.0 tickets\n",
      "1 21.0 ticket\n",
      "1 20.0 see\n",
      "1 20.0 guys\n",
      "1 20.0 going\n",
      "1 20.0 follow\n",
      "1 20.0 chance\n",
      "1 20.0 back\n",
      "1 18.0 still\n",
      "1 18.0 service\n",
      "1 18.0 next\n",
      "1 18.0 dfw\n",
      "1 17.0 use\n",
      "1 17.0 think\n",
      "1 17.0 show\n",
      "1 17.0 reservation\n",
      "1 17.0 first\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1344.0 @\n",
      "2 1051.0 !\n",
      "2 722.0 .\n",
      "2 328.0 #\n",
      "2 311.0 southwestair\n",
      "2 292.0 jetblue\n",
      "2 285.0 thanks\n",
      "2 256.0 thank\n",
      "2 252.0 united\n",
      "2 245.0 ``\n",
      "2 182.0 :\n",
      "2 176.0 flight\n",
      "2 173.0 americanair\n",
      "2 141.0 usairways\n",
      "2 138.0 great\n",
      "2 90.0 )\n",
      "2 89.0 service\n",
      "2 79.0 http\n",
      "2 75.0 virginamerica\n",
      "2 71.0 best\n",
      "2 69.0 love\n",
      "2 66.0 much\n",
      "2 66.0 customer\n",
      "2 61.0 guys\n",
      "2 60.0 awesome\n",
      "2 55.0 ;\n",
      "2 53.0 's\n",
      "2 52.0 -\n",
      "2 51.0 good\n",
      "2 47.0 got\n",
      "2 47.0 airline\n",
      "2 45.0 us\n",
      "2 44.0 time\n",
      "2 44.0 n't\n",
      "2 38.0 today\n",
      "2 38.0 crew\n",
      "2 37.0 get\n",
      "2 36.0 amazing\n",
      "2 35.0 &\n",
      "2 33.0 help\n",
      "2 33.0 flying\n",
      "2 31.0 fly\n",
      "2 31.0 ...\n",
      "2 30.0 appreciate\n",
      "2 29.0 response\n",
      "2 29.0 amp\n",
      "2 28.0 made\n",
      "2 28.0 ever\n",
      "2 28.0 back\n",
      "2 27.0 work\n",
      "2 27.0 're\n",
      "2 25.0 see\n",
      "2 25.0 plane\n",
      "2 25.0 home\n",
      "2 24.0 tonight\n",
      "2 24.0 new\n",
      "2 24.0 gate\n",
      "2 24.0 ?\n",
      "2 23.0 yes\n",
      "2 23.0 u\n",
      "2 23.0 southwest\n",
      "2 23.0 flights\n",
      "2 23.0 'm\n",
      "2 23.0 ''\n",
      "2 22.0 well\n",
      "2 22.0 like\n",
      "2 21.0 team\n",
      "2 21.0 helpful\n",
      "2 21.0 always\n",
      "2 20.0 please\n",
      "2 20.0 nice\n",
      "2 20.0 know\n",
      "2 20.0 happy\n",
      "2 20.0 first\n",
      "2 20.0 day\n",
      "2 20.0 attendant\n",
      "2 19.0 way\n",
      "2 19.0 staff\n",
      "2 19.0 class\n",
      "2 18.0 wait\n"
     ]
    }
   ],
   "source": [
    "train_model('bag', 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Which features did you expect for each separate class and why?\n",
    "For negative, we expected words such as 'cancelled', 'delayed', and 'late' because those directly express dissatisfaction. We also expected domain nouns like 'flight' and 'bag', since complaints are often about baggage or other services. For neutral, we expected airline handles and names, because neutral tweets tend to be inquiries addressed to the companies. For positive, we expected appraisal terms such as 'thanks' and 'awesome', because they clearly signal positive sentiment.\n",
    "\n",
    "Which features did you not expect and why?\n",
    "We were surprised by the number of punctuation tokens and numbers, such as '@' and '#', since they reflect tweet structure rather than sentiment. Finally, we didn't expect to see job-related nouns like 'crew' or 'attendant' in the positive list, as they're relevant to the context but not inherently positive. Perhaps it is because most positive tweets are about the employees, while negative ones are about the company.\n",
    "\n",
    "Which words would you remove or keep when trying to improve the model and why?\n",
    "We would remove punctuation tokens, URL tokens ('http'), conjunction artifacts ('amp'), standalone numbers, and other similar tokens, since they add noise without carrying any sentiment. We'd also drop airline‐specific names because they introduce bias, and don't have an inherent sentiment. We would keep core sentiment words and strong negation words, because they directly indicate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣤⣤⣤⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⡿⠛⠉⠙⠛⠛⠛⠛⠻⢿⣿⣷⣤⡀⠀⠀⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⠋⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠈⢻⣿⣿⡄⠀⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⣸⣿⡏⠀⠀⠀⣠⣶⣾⣿⣿⣿⠿⠿⠿⢿⣿⣿⣿⣄⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣿⠁⠀⠀⢰⣿⣿⣯⠁⠀⠀⠀⠀⠀⠀⠀⠈⠙⢿⣷⡄⠀ \n",
    "⠀⠀⣀⣤⣴⣶⣶⣿⡟⠀⠀⠀⢸⣿⣿⣿⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣷⠀ \n",
    "⠀⢰⣿⡟⠋⠉⣹⣿⡇⠀⠀⠀⠘⣿⣿⣿⣿⣷⣦⣤⣤⣤⣶⣶⣶⣶⣿⣿⣿⠀ \n",
    "⠀⢸⣿⡇⠀⠀⣿⣿⡇⠀⠀⠀⠀⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⠀ \n",
    "⠀⣸⣿⡇⠀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠉⠻⠿⣿⣿⣿⣿⡿⠿⠿⠛⢻⣿⡇⠀⠀ \n",
    "⠀⣿⣿⠁⠀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣧⠀⠀ \n",
    "⠀⣿⣿⠀⠀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⠀ \n",
    "⠀⣿⣿⠀⠀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⠀ \n",
    "⠀⢿⣿⡆⠀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⡇⠀⠀ \n",
    "⠀⠸⣿⣧⡀⠀⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⠃⠀⠀ \n",
    "⠀⠀⠛⢿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⣰⣿⣿⣷⣶⣶⣶⣶⠶⠀⢠⣿⣿⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣿⠀⠀⠀⠀⠀⣿⣿⡇⠀⣽⣿⡏⠁⠀⠀⢸⣿⡇⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣿⠀⠀⠀⠀⠀⣿⣿⡇⠀⢹⣿⡆⠀⠀⠀⣸⣿⠇⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⢿⣿⣦⣄⣀⣠⣴⣿⣿⠁⠀⠈⠻⣿⣿⣿⣿⡿⠏⠀⠀⠀⠀ \n",
    "⠀⠀⠀⠀⠀⠀⠀⠈⠛⠻⠿⠿⠿⠿⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
